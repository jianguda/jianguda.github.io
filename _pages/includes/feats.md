# ðŸ’» Featured Work

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICSE'26 @ Rio de Janeiro</div><img src='images/featured-spot.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Semantic-based Optimization for Repairing LLMs: Case Study on Code Generation](https://arxiv.org/abs/2503.12899) \\
**Jian Gu**, Aldeida Aleti, Chunyang Chen, Hongyu Zhang

STAR is a novel semantic-based optimization approach for LM repair that efficiently locates and patches buggy neurons using statistical insights and analytical formulas, outperforming prior methods in effectiveness, efficiency, and minimizing side effects.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL'25 @ Vienna</div><img src='images/featured-vst.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Semantic-Aware Layer-Freezing for Computation-Efficient Fine-Tuning of LMs](https://arxiv.org/abs/2406.11753) \\
**Jian Gu**, Aldeida Aleti, Chunyang Chen, Hongyu Zhang

Our semantic-based layer freezing approach improves the efficiency of language model finetuning by determining where to finetune, outperforming existing methods through a detailed semantic analysis of the model's inference process.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><img src='images/featured-vds.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Vocabulary-Defined Semantics: Latent Space Clustering for Beyond-Context Learning](https://arxiv.org/abs/2401.16184) \\
**Jian Gu**, Aldeida Aleti, Chunyang Chen, Hongyu Zhang

<!-- Vocabulary-defined Semantics enhances language model understanding and adaptability through disentangled semantic analysis, improved logits computation, and neural clustering, outperforming existing techniques in diverse text understanding tasks. -->
We propose "vocabulary-defined semantics" to reformulate in-context learning as a clustering problem, aligning semantic properties of language models with downstream data, outperforming state-of-the-art methods in effectiveness, efficiency and robustness.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TOSEM 2026</div><img src='images/featured-mint.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Neuron Patching: Semantic-based Neuron-level LM Repair for Code Generation](https://arxiv.org/abs/2312.05356) \\
**Jian Gu**, Aldeida Aleti, Chunyang Chen, Hongyu Zhang

MINT is an efficient and reliable technique for repairing large language models in software engineering. It can successfully solve model failures by patching merely 1 or 2 neurons, outperforming state-of-the-art methods in coding tasks.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">FSE'23 @ San Francisco</div><img src='images/featured-asd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables](https://dl.acm.org/doi/abs/10.1145/3611643.3613076) \\
**Jian Gu**, Harald C. Gall

Deep code generation integrates neural models into software engineering for generating code but requires enhancements for project-level tasks, suggesting a taxonomy on code data and introducing a semantic pyramid framework to improve software development processes.

</div>
</div>
